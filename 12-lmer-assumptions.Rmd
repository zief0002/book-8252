 # Linear Mixed-Effects Models: Alternative Representations and Assumptions {#lmer-longitudinal} -->

<!-- ```{r echo=FALSE, message=FALSE} -->
<!-- library(knitr) -->
<!-- library(kableExtra) -->

<!-- opts_knit$set( -->
<!--   width = 85, -->
<!--   tibble.print_max = Inf -->
<!--   ) -->

<!-- opts_chunk$set( -->
<!--   prompt = FALSE, -->
<!--   comment = NA, -->
<!--   message = FALSE, -->
<!--   warning = FALSE, -->
<!--   tidy = FALSE, -->
<!--   fig.align = 'center', -->
<!--   out.width = '50%' -->
<!--   ) -->
<!-- ``` -->



<!-- In this set of notes, you will learn alternative ways of representing the linear mixed-effects model. You will also learn about the underlying assumptions for the linear mixed-effects model, as well as how to evaluate them empirically. -->

<!-- --- -->

<!-- <!-- ### Preparation {-} --> -->

<!-- <!-- Before class you will need to read: --> -->

<!-- <!-- - Uchikoshi, Y. (2005). [Narrative development in bilingual kindergarteners: Can Arthur help?](http://psycnet.apa.org.ezp3.lib.umn.edu/journals/dev/41/3/464) *Developmental Psychology, 41*(3), 464&ndash;478. doi: 10.1037/0012-1649.41.3.464 --> -->


<!-- <!-- <br /> --> -->

<!-- <!-- --- --> -->



<!-- ## Dataset and Research Question -->

<!-- In this set of notes, we will use data from two files, the *netherlands-students.csv* file and the *netherlands-schools.csv* file (see the [data codebook](#netherlands) here). These data include student- and school-level attributes, respectively, for $n_i=2287$ 8th-grade students in the Netherlands. -->

<!-- ```{r message=FALSE, paged.print=FALSE} -->
<!-- # Load libraries -->
<!-- library(AICcmodavg) -->
<!-- library(broom) -->
<!-- library(dplyr) -->
<!-- library(ggplot2) -->
<!-- library(lme4) #for fitting mixed-effects models -->
<!-- library(readr) -->
<!-- library(sm) -->
<!-- library(tidyr) -->

<!-- # Read in student-level data -->
<!-- student_data = read_csv(file = "~/Documents/github/epsy-8252/data/netherlands-students.csv") -->

<!-- # Read in school-level data -->
<!-- school_data = read_csv(file = "~/Documents/github/epsy-8252/data/netherlands-schools.csv") -->

<!-- # Join the two datasets together -->
<!-- joined_data = left_join(student_data, school_data, by = "school_id") -->
<!-- head(joined_data) -->
<!-- ``` -->

<!-- We will use these data to explore the question of whether verbal IQ scores predict variation in post-test language scores. -->


<!-- ## Statistical Models -->

<!-- In the last unit we fitted several linear mixed-effects models to predict variation in students' post-test language scores. The statistical models for these are presented below. -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \mathbf{Model~1:~}\mathrm{Language~Score}_{ij} &= \big[\beta_0 + b_{0j}\big] + \epsilon_{ij} \\[1em] -->
<!-- \mathbf{Model~2:~}\mathrm{Language~Score}_{ij} &= \big[\beta_0 + b_{0j}\big] + \beta_1(\mathrm{Verbal~IQ}_{ij}) + \epsilon_{ij} \\[1em] -->
<!-- \mathbf{Model~3:~}\mathrm{Language~Score}_{ij} &= \big[\beta_0 + b_{0j}\big] + \beta_1(\mathrm{Verbal~IQ}_{ij}) + \beta_2(\mathrm{Pre\mbox{-}test~score}_{ij}) + \beta_3(\mathrm{SES}_{ij}) + \epsilon_{ij} \\[1em] -->
<!-- \mathbf{Model~4:~}\mathrm{Language~Score}_{ij} &= \big[\beta_0 + b_{0j}\big] + \beta_1(\mathrm{Verbal~IQ}_{ij}) + \beta_2(\mathrm{Pre\mbox{-}test~score}_{ij}) + \beta_3(\mathrm{SES}_{ij}) + \\ -->
<!-- &\qquad\beta_4(\mathrm{Non\mbox{-}denominational}_{\bullet j}) + \beta_5(\mathrm{Protestant}_{\bullet j}) + \beta_6(\mathrm{Public}_{\bullet j})  + \epsilon_{ij} -->
<!-- \end{split} -->
<!-- $$ -->

<!-- This representation of the models is referred to as the **composite model** or the **mixed-effects model** since they include both the fixed- and random-effects in the same equation. -->


<!-- ## Writing the Model as a Multilevel Model -->

<!-- Another way we can express the model is by seperating the mixed-effects model equation into multiple equations; one for each level of variation. For example, each of the mixed-effects models listed above could be separated into two equations: a student-level equation (Level-1) and a set of school-level equations (Level-2). As an example, take the equation for Model 2: -->

<!-- $$ -->
<!-- \mathrm{Language~Score}_{ij} = \big[\beta_0 + b_{0j}\big] + \beta_1(\mathrm{Verbal~IQ}_{ij}) + \epsilon_{ij} -->
<!-- $$ -->

<!-- We initially write the student-level, or Level-1 equation. The level-1 equation includes all the fixed-effects and the random error term from the mixed-effects model. When writing the Level-1 model, we add a *j* subscript to each of the fixed-effects to indicate that the particular effect may be unique to a particular school. The Level-1 equation for Model 2 is: -->

<!-- $$ -->
<!-- \mathrm{Language~Score}_{ij} = \beta_{0j} + \beta_{1j}(\mathrm{Verbal~IQ}_{ij}) + \epsilon_{ij} -->
<!-- $$ -->

<!-- The Level-1 equation describes the variation in students' post-test language scores. It says that this variation is decomposed into that which is explained by differences in students' verbal IQ scores and unexplained random error. The *j* subscipt on the fixed-effects terms indicates that the intercept and effect of verbal IQ scores is the same for all students within a particular school. -->

<!-- After writing the Level-1 equation, we can write out the Level-2, or school-level equation(s). There will be a Level-2 equation for each of the fixed-effects in the Level-1 model. In our example, since we have two fixed-effects in the Level-1 model ($\beta_0$ and $\beta_1$), there will be two Level-2 equations. In each Level-2 equations, the outcome is one of the fixed-effects from the Level-1 equation. These equations describe how the school-specific intercept and slopes differ across schools. As such, they may include the random-effects and any school-level effects. For example, we can write the Level-2 equations for Model 2 as: -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \beta_{0j} &= \beta_0 + b_{0j}\\ -->
<!-- \beta_{1j} &= \beta_1\\ -->
<!-- \end{split} -->
<!-- $$ -->

<!-- These equations indicate that the school-specific intercepts are a function of some part common to all schools ($\beta_0$; a fixed-effect) and some deviation from that ($b_{0j}$; a random-effect of intercept). The random-effect is the reason that schools can have different intercepts. The school-specific slope, on the other hand, dos not vary by school; it is the same for each school. -->

<!-- Together these equations are referred to as the set of multilevel equations: -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \mathbf{Level\mbox{-}1:}\\ -->
<!-- &~ \mathrm{Language~Score}_{ij} = \beta_{0j} + \beta_{1j}(\mathrm{Verbal~IQ}_{ij}) + \epsilon_{ij}\\ -->
<!-- \mathbf{Level\mbox{-}2:}\\ -->
<!-- &~ \beta_{0j} = \beta_0 + b_{0j}\\ -->
<!-- &~ \beta_{1j} = \beta_1\\ -->
<!-- \end{split} -->
<!-- $$ -->

<!-- ### Model 4 as a Multilevel Equation -->

<!-- As a second example, consider Model 4: -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \mathrm{Language~Score}_{ij} &= \big[\beta_0 + b_{0j}\big] + \beta_1(\mathrm{Verbal~IQ}_{ij}) + \beta_2(\mathrm{Pre\mbox{-}test~score}_{ij}) + \beta_3(\mathrm{SES}_{ij}) + \\ -->
<!-- &\qquad\beta_4(\mathrm{Non\mbox{-}denominational}_{\bullet j}) + \beta_5(\mathrm{Protestant}_{\bullet j}) + \beta_6(\mathrm{Public}_{\bullet j})  + \epsilon_{ij} -->
<!-- \end{split} -->
<!-- $$ -->

<!-- The Level-1 equation is: -->

<!-- $$ -->
<!-- \mathrm{Language~Score}_{ij} = \beta_{0j} + \beta_{1j}(\mathrm{Verbal~IQ}_{ij}) + \beta_{2j}(\mathrm{Pre\mbox{-}test~score}_{ij}) + \beta_{3j}(\mathrm{SES}_{ij}) + \epsilon_{ij} -->
<!-- $$ -->

<!-- This equation indicates that students' post-test language scores are a function of a school-specific intercept, a school-specific effect of verbal IQ scores, a school-specific effect of language pre-test scores, a school-specific effect of SES, and random error. There are now four Level-2 equations: -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \beta_{0j} &= \beta_0 + \beta_4(\mathrm{Non\mbox{-}denominational}_{\bullet j}) + \beta_5(\mathrm{Protestant}_{\bullet j}) + \beta_6(\mathrm{Public}_{\bullet j}) + b_{0j}\\ -->
<!-- \beta_{1j} &= \beta_1\\ -->
<!-- \beta_{2j} &= \beta_2\\ -->
<!-- \beta_{3j} &= \beta_3\\ -->
<!-- \end{split} -->
<!-- $$ -->

<!-- These equations indicates that each school-specific intercept is a function of a fixed, or common, intercept, the type of school, and random error. The effects of verbal IQ scores, pre-test language scores, and SES are constant, or fixed, across schools (there is not a random-effect in any of the last three Level-2 equations). -->


<!-- ### Going from Multilevel Models to the Mixed-Effects Model -->

<!-- If we have the multilevel equations, we can substitute the Level-2 equation(s) into the Level-1 equation to get the composite equation or mixed-effects equation. For example, for Model 2, substituting $\beta_0 + b_{0j}$ into $\beta_{0j}$ and $\beta_1$ into $\beta_{1j}$ gives us: -->

<!-- $$ -->
<!-- \mathrm{Language~Score}_{ij} = \beta_{0} + b_{0j} + \beta_{1}(\mathrm{Verbal~IQ}_{ij}) + \epsilon_{ij} -->
<!-- $$ -->

<!-- Similarly substituting the Level-2 information into the Level-1 equation for Model 4, we end up the following mixed-effects represenation: -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \mathrm{Language~Score}_{ij} &= \bigg[\beta_0 + \beta_4(\mathrm{Non\mbox{-}denominational}_{\bullet j}) + \beta_5(\mathrm{Protestant}_{\bullet j}) + \beta_6(\mathrm{Public}_{\bullet j}) + b_{0j}\bigg] +  \\ -->
<!-- &\qquad \beta_1(\mathrm{Verbal~IQ}_{ij}) + \beta_2(\mathrm{Pre\mbox{-}test~score}_{ij}) + \beta_3(\mathrm{SES}_{ij}) +  + \epsilon_{ij} -->
<!-- \end{split} -->
<!-- $$ -->

<!-- Which, if we re-arrange terms gives us the same mixed-effects model that we started with. This substitution also helps us think about which Level-2 equation we put the the school-level predictors in. For example, what if we would have put the school type predictors into the Level-2 equation associated with verbal IQ? -->


<!-- $$ -->
<!-- \begin{split} -->
<!-- \mathbf{Level\mbox{-}1:}\\ -->
<!-- &~ \mathrm{Language~Score}_{ij} = \beta_{0j} + \beta_{1j}(\mathrm{Verbal~IQ}_{ij}) + \beta_{2j}(\mathrm{Pre\mbox{-}test~score}_{ij}) + \beta_{3j}(\mathrm{SES}_{ij}) + \epsilon_{ij}\\ -->
<!-- \mathbf{Level\mbox{-}2:}\\ -->
<!-- &~ \beta_{0j} = \beta_0 + b_{0j}\\ -->
<!-- &~ \beta_{1j} = \beta_1 + \beta_4(\mathrm{Non\mbox{-}denominational}_{\bullet j}) + \beta_5(\mathrm{Protestant}_{\bullet j}) + \beta_6(\mathrm{Public}_{\bullet j})\\ -->
<!-- &~ \beta_{2j} = \beta_2\\ -->
<!-- &~ \beta_{3j} = \beta_3\\ -->
<!-- \end{split} -->
<!-- $$ -->

<!-- If we substitute back into the Level-1 equation, the composite equation is: -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \mathrm{Language~Score}_{ij} &= \bigg[\beta_0 + b_{0j}\bigg] +  \\ -->
<!-- &\qquad \bigg[\beta_1 + \beta_4(\mathrm{Non\mbox{-}denominational}_{\bullet j}) + \beta_5(\mathrm{Protestant}_{\bullet j}) + \beta_6(\mathrm{Public}_{\bullet j}) \bigg](\mathrm{Verbal~IQ}_{ij}) + \\ -->
<!-- &\qquad \beta_2(\mathrm{Pre\mbox{-}test~score}_{ij}) + \beta_3(\mathrm{SES}_{ij}) + \epsilon_{ij} \\[1em] -->
<!-- &= \big[\beta_0 + b_{0j}\big] + \beta_1(\mathrm{Verbal~IQ}_{ij}) + \beta_4(\mathrm{Non\mbox{-}denominational}_{\bullet j})(\mathrm{Verbal~IQ}_{ij}) + \\  -->
<!-- &\qquad \beta_5(\mathrm{Protestant}_{\bullet j})(\mathrm{Verbal~IQ}_{ij}) + \beta_6(\mathrm{Public}_{\bullet j})(\mathrm{Verbal~IQ}_{ij}) + \beta_2(\mathrm{Pre\mbox{-}test~score}_{ij}) + \\ -->
<!-- &\qquad \beta_3(\mathrm{SES}_{ij}) + \epsilon_{ij} -->
<!-- \end{split} -->
<!-- $$ -->

<!-- Adding the school type predictors in the verbal IQ equation produces interaction terms (product terms) in the composite equation! Adding them to the intercept equation produced main-effects. Since our orginal mixed-effects model included school type as a main-effect, we need to include them in the intercept equation. Note that if the composite equation included both a main-effect and an interaction, we would need to include the predictor in more than one of the Level-2 equations (e.g., in the intercept equation and the verbal IQ equation). -->

<!-- > Any predictors included in the Level-2 slope equations also need to be included in the Level-2 intercept equation. -->


<!-- ### Guidelines for Writing the Multilevel Equations -->

<!-- Here are some guidelines in helping you think about writing multilevel equations.  -->

<!-- - Write the Level-1 equation first. This will be an equation that expresses the outcome's relationship to a series of school-specific parameters and a student-specific residual. -->
<!-- - The number of school-specific parameters in the Level-1 equation (aside from the residual) dictate the number of Level-2 equations you will have.  -->
<!-- - The school-specific parameters from the Level-1 equation will be the outcomes in the Level-2 equations. -->
<!-- - Random-effects are the residuals in the Level-2 equations, and therefore are in the Level-2 equations; one per equation. -->
<!-- - Variables from the data go to their appropriate level. For example student-level variables will be put in the Level-1 equation, and school-level predictors will be put in one or more of the Level-2 equations. -->


<!-- ## Multilevel Equations for Fixed-Effects Models -->

<!-- Our conventional fixed-effects regression models (LM) can also be expressed as a multilevel model. For example, consider the fixed-effect model that includes an intercept and effect of verbal IQ score: -->

<!-- $$ -->
<!-- \mathrm{Language~Score}_{i} = \beta_{0} + \beta_{1}(\mathrm{Verbal~IQ}_{i}) + \epsilon_{i} -->
<!-- $$ -->

<!-- The multilevel model would specify that the Level-2 equations would only include fixed-effects (no random-effects). Thus when we substitute them back into the Level-1 model we only have fixed-effects in the model: -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \mathbf{Level\mbox{-}1:}\\ -->
<!-- &~ \mathrm{Language~Score}_{ij} = \beta_{0j} + \beta_{1j}(\mathrm{Verbal~IQ}_{ij}) + \epsilon_{ij}\\ -->
<!-- \mathbf{Level\mbox{-}2:}\\ -->
<!-- &~ \beta_{0j} = \beta_{0}\\ -->
<!-- &~ \beta_{1j} = \beta_{1} -->
<!-- \end{split} -->
<!-- $$ -->

<!-- ## Why are Multilevel Expressions Helpful? -->

<!-- Expressing the model as a set of multilevel equations can be helpful for readers. First, it explicitly separates the sources of variation and the predictors of these sources of variation into different levels. In our example there are two sources of variation student-level variation (within-school) and school-level variation (between-school). The Level-1 model attempts to describe the student-level variation and only includes predictors of the student-level variation (i.e., only student-level predictors appear in the Level-1 model). The Level-2 models attempts to describe the school-level variation and only includes predictors of the school-level variation (i.e., only school-level predictors appear in the Level-2 models). -->

<!-- Secondly, the multilevel expression of the model helps us think aboout what the predictors at each level are actually doing. Level-1 predictors explain variation in the outcome. In our example, they are explaining variation in students' post-test language scores. The Level-2 predictors are explaining variation in the school-specific intercepts (or intercepts and slopes)---they explain Level-2 variation.  -->

<!-- Thirdly, the multilevel expression of the model helps us see that the random-effects are residuals; they are residuals of the Level-2 models. This helps us think about the more general statistical model. For example, the general statistial model for a model that includes fixed-effects of two predictors and a random-effect of intercept is: -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \mathbf{Level\mbox{-}1:}\\ -->
<!-- &~ Y_{ij} = \beta_{0j} + \beta_{1j}(X_{1ij}) + \beta_{2j}(X_{2ij})  + \epsilon_{ij}\\ -->
<!-- \mathbf{Level\mbox{-}2:}\\ -->
<!-- &~ \beta_{0j} = \beta_{0} + b_{0j}\\ -->
<!-- &~ \beta_{1j} = \beta_{1} \\ -->
<!-- &~ \beta_{2j} = \beta_{2} -->
<!-- \end{split} -->
<!-- $$ -->

<!-- where -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \epsilon_{ij} &\sim \mathcal{N}\bigg(0,\sigma^2_{\epsilon}\bigg)\\[1em] -->
<!-- b_{0j} &\sim \mathcal{N}\bigg(0,\sigma^2_{0}\bigg) -->
<!-- \end{split} -->
<!-- $$ -->

<!-- In the mixed-effects model we put distributional assumptions on both the Level-1 residuals and the Level-2 residuals.  -->

<!-- ### Evaluating the Assumptions: An Example -->

<!-- Evaluating the assumptions in a mixed-effects model is a bit more complicated than it is in a fixed-effects model, and there are several things to check depending on the model that was fitted (among other things, the number of random-effects and their covariance structure). To simplify things, in this course, we will evaluate the distributional assumptions placed on the Level-1 residuals, and we will also evaluate the normality assumption on the random-effects. -->

<!-- To illustrate assumption checking in practice, we will evaluate the assumptions for fitting Model 4. Recall that the multilevel expression of Model 4 was: -->

<!-- $$ -->
<!-- \begin{split} -->
<!-- \mathbf{Level\mbox{-}1:}\\ -->
<!-- &~ \mathrm{Language~Score}_{ij} = \beta_{0j} + \beta_{1j}(\mathrm{Verbal~IQ}_{ij}) + \beta_{2j}(\mathrm{Pre\mbox{-}test~score}_{ij}) + \beta_{3j}(\mathrm{SES}_{ij}) + \epsilon_{ij}\\ -->
<!-- \mathbf{Level\mbox{-}2:}\\ -->
<!-- &~ \beta_{0j} = \beta_0 + \beta_4(\mathrm{Non\mbox{-}denominational}_{\bullet j}) + \beta_5(\mathrm{Protestant}_{\bullet j}) + \beta_6(\mathrm{Public}_{\bullet j}) +b_{0j}\\ -->
<!-- &~ \beta_{1j} = \beta_1 \\ -->
<!-- &~ \beta_{2j} = \beta_2\\ -->
<!-- &~ \beta_{3j} = \beta_3\\ -->
<!-- \end{split} -->
<!-- $$ -->

<!-- The assumptions are based on the Level-1 residuals ($\epsilon_{ij}$) and the Level-2 residuals, or random-effects ($b_{0j}$). So we need to examine the distributions of those two components. To begin, we will fit the mixed-effects model. -->

<!-- ```{r} -->
<!-- # Fit Model 4 -->
<!-- lmer.4 = lmer(language_post ~ 1 + language_pre + ses +  -->
<!--                 school_type + verbal_iq + (1 | school_id),  -->
<!--               data = joined_data, REML = FALSE) -->
<!-- ``` -->


<!-- ### Evaluate Assumptions about the Level-1 Residuals -->

<!-- We will evaluate the Level-1 residuals in the exact same way we evalauted the residuals from a fixed-effects (LM) analysis. The `augment()` function from the **broom** package produces the Level-1 residuals and fitted values. -->

<!-- ```{r} -->
<!-- # Augment the model to get the Level-1 residuals and fitted values -->
<!-- out_4 = augment(lmer.4) -->
<!-- head(out_4) -->
<!-- ``` -->

<!-- The Level-1 residuals are found in the `.resid` column, and the `.fitted` column contains the $\hat{Y}$ values. As with LM residual analysis, we want to examine the normality of the residuals in a density plot (or some other plot that allows you to evaluate this), and the other assumptions by plotting the residuals against the fitted values in a scatterplot. -->

<!-- ```{r fig.width=6, fig.height=6, out.width='40%', fig.show='hold', fig.cap='Plots to evaluate the Level-1 residuals.'} -->
<!-- # Density plot of the level-1 residuals -->
<!-- sm.density(out_4$.resid, model = "normal") -->

<!-- # Scatterplot of the Level-1 residuals versus the fitted values -->
<!-- ggplot(data = out_4, aes(x = .fitted, y = .resid)) + -->
<!--   geom_point() + -->
<!--   geom_hline(yintercept = 0) + -->
<!--   theme_bw() + -->
<!--   xlab("Fitted values") + -->
<!--   ylab("Level-1 residuals") -->
<!-- ``` -->



<!-- Based on the plots, some of the distributional assumptions for the Level-1 residuals seem reasonably satisfied. The density plot suggests that the normality assumption is tenable, and the scatterplot shows symmetry around the $Y=0$ line (linearity). The assumption of homoskedasticity is more in question. The pattern in the residuals shows more variation for fitted values between 30 and 50, and less variation for smaller and larger fitted values. -->


<!-- ### Assumptions about the Random-Effects -->

<!-- We also need to examine the assumptions for any random-effects included in the model. For this course, we will examine the normaility assumption. In our example that means we need to examine the normality assumption about the intercept random-effects. To do this we need to extract the random-effects from the model so we can use the `sm.density()` function to evaluate normality. -->


<!-- ```{r out.width='45%', fig.cap='Density plot of the estimated random-effects for intercept.'} -->
<!-- # Obtain the RE for intercept and slope -->
<!-- re_int = ranef(lmer.4)$school_id[ , 1] -->

<!-- # Density plot of the RE for intercept -->
<!-- sm.density(re_int, model = "normal", xlab = "RE for intercept") -->
<!-- ``` -->


<!-- This assumption looks reasonably satisfied.  -->




<!-- ## Other Resources {-} -->

<!-- In addition to the notes and what we cover in class, there many other resources for learning about using linear mixed-effects models for longitudinal analysis. Here are some resources that may be helpful in that endeavor: -->

<!-- - Long, J. D. (2012). [Longitudinal data analysis for the behavioral sciences using R](http://www.amazon.com/Longitudinal-Analysis-Behavioral-Sciences-Using/dp/1412982685). Thousand Oaks, CA: Sage. -->
<!-- - Swihart, B. J., Caffo, B., James, B. D., Strand, M., Schwartz, B. S., &amp; Punjabi, N. M. (2010). [Lasagna plots: A saucy alternative to spaghetti plots.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2937254/) *Epidemiology, 21*(5), 621&ndash;625. -->


